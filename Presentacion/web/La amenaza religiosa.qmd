---
title: "La amenaza religiosa y el orden estatal en el discurso oficial mexicano (1926‚Äì1928): un an√°lisis exploratorio con t√©cnicas de miner√≠a de texto"
author: "Emanuel Gagnolo"
date: "2025-06-06"
format: 
  html:
    lang: es
    toc: true
    toc-location: left
    citation: true
    csl: apa.csl  # <- estilo APA
    link-citations: true
bibliography: referencias.bib
editor: visual
---

```{r setup, include = FALSE}
# Cargar paquetes necesarios
require(pacman)
pacman::p_load(htmltools,tidyverse,readr,tidytext,stringr,reactable,ggplot2,plotly,textdata,
               DT,widyr,stringi,knitr,stopwords,RColorBrewer,wordcloud,scales,
               corrplot,gridExtra,lubridate,tm,SnowballC,textclean,patchwork,ggraph,
               igraph,quanteda,quanteda.textplots,topicmodels,cluster,factoextra,
               VIM,car,broom,ComplexHeatmap,circlize,wesanderson,ggraph)

```

## Introducci√≥n al caso de estudio

Entre 1926 y 1928, M√©xico atraves√≥ uno de los conflictos m√°s significativos entre el Estado y la Iglesia cat√≥lica, conocido como la Guerra Cristera. La promulgaci√≥n de la denominada Ley Calles -una normativa federal que impon√≠a severas restricciones al culto religioso, como la prohibici√≥n de la participaci√≥n pol√≠tica del clero y la limitaci√≥n del n√∫mero de ministros- intensific√≥ las tensiones entre ambos poderes y deriv√≥ en enfrentamientos armados. En ese contexto, el gobierno de Plutarco El√≠as Calles produjo discursos y documentos oficiales de alto impacto pol√≠tico, en los que justificaba p√∫blicamente sus decisiones y delineaba una visi√≥n de la autoridad gubernamental en oposici√≥n al poder de la Iglesia cat√≥lica. Como se√±ala @velasco2015estado, la afirmaci√≥n del Estado laico en M√©xico no fue solo un principio normativo, sino una estrategia activa de delimitaci√≥n frente a la injerencia eclesi√°stica.

Este trabajo propone analizar dichos textos desde una perspectiva sociol√≥gica y computacional, con el fin de indagar c√≥mo el Estado construy√≥ discursivamente la idea de la religi√≥n como una amenaza al orden estatal.

## Objetivo general

Analizar las representaciones discursivas de la amenaza religiosa y del orden estatal en decretos y mensajes oficiales emitidos durante la vigencia de la Ley Calles (1926‚Äì1928), mediante herramientas de miner√≠a de texto.

### Objetivos espec√≠ficos

-   Identificar los t√©rminos m√°s frecuentes vinculados a los conceptos de ‚Äúreligi√≥n‚Äù, ‚Äúamenaza‚Äù y ‚Äúorden‚Äù en los documentos oficiales del per√≠odo.
-   Analizar c√≥mo estas representaciones se articulan con estrategias discursivas de legitimaci√≥n del poder estatal.
-   Examinar el tono emocional predominante en los discursos y su posible evoluci√≥n entre 1926 y 1928, a fin de explorar sus efectos en la construcci√≥n de sentido sobre la conflictividad religiosa.

## Pregunta de investigaci√≥n

¬øC√≥mo construy√≥ el discurso estatal mexicano representaciones de la amenaza religiosa y del orden durante la aplicaci√≥n de la Ley Calles (1926‚Äì1928)?

### Subpreguntas:

-   ¬øQu√© t√©rminos y asociaciones sem√°nticas se reiteran en los discursos oficiales frente al conflicto religioso?
-   ¬øC√≥mo var√≠a el enfoque discursivo seg√∫n el tipo de documento?

## Hip√≥tesis

El discurso oficial producido entre 1926 y 1928 por el gobierno de Plutarco El√≠as Calles construy√≥ a la religi√≥n cat√≥lica como una amenaza al orden p√∫blico, mediante recursos ret√≥ricos que asociaron a sus actores con pr√°cticas sediciosas, fan√°ticas o desobedientes. Esta estrategia discursiva configur√≥ un proceso de criminalizaci√≥n simb√≥lica que busc√≥ legitimar el ejercicio del poder estatal y reforzar su autoridad frente al conflicto religioso.

## Metodolog√≠a

La investigaci√≥n adopta un enfoque exploratorio y cualitativo, complementado con t√©cnicas computacionales propias del an√°lisis de texto. Los documentos ser√°n ingresados directamente en el entorno de trabajo en R en formato textual. Se aplicar√°n procesos de preprocesamiento como tokenizaci√≥n, normalizaci√≥n, eliminaci√≥n de palabras vac√≠as y lematizaci√≥n. Luego se realizar√°n an√°lisis de frecuencia, coocurrencias, visualizaci√≥n con nubes de palabras y redes sem√°nticas. Tambi√©n se prev√© realizar un an√°lisis del tono discursivo mediante t√©cnicas de an√°lisis de sentimientos y etiquetado gramatical (POS tagging). Este abordaje permitir√° identificar patrones discursivos no evidentes a simple vista e interpretar las estrategias simb√≥licas del Estado frente al conflicto religioso.

## Corpus y fuentes

El corpus estar√° conformado por cuatro documentos oficiales emitidos por el Poder Ejecutivo mexicano entre 1926 y 1928: decretos presidenciales, discursos p√∫blicos, mensajes y cartas firmadas por el presidente Plutarco El√≠as Calles. Los textos ser√°n seleccionados a partir de repositorios digitales p√∫blicos y confiables:

-   Memoria Pol√≠tica de M√©xico: Sitio especializado en discursos, leyes y documentos presidenciales. (http://www.memoriapoliticademexico.org)

-   

    ```         
    Hemeroteca Digital Nacional de M√©xico (UNAM): Archivo de prensa hist√≥rica que incluye la reproducci√≥n de documentos oficiales.(https://hndm.iib.unam.mx)
    ```

-   

    ```         
    Diario Oficial de la Federaci√≥n (DOF): Publicaci√≥n oficial del Estado donde se registran los decretos presidenciales. (https://www.dof.gob.mx)
    ```

-   Archivo General de la Naci√≥n (AGN): Repositorio hist√≥rico que conserva documentos originales de gobierno. (https://www.gob.mx/agn)

Las variables consideradas ser√°n: a√±o, tipo de documento, extensi√≥n, frecuencia de t√©rminos clave, coocurrencias l√©xicas y tono emocional estimado. La elecci√≥n de este corpus responde a su valor como fuente primaria para estudiar la construcci√≥n discursiva del poder estatal frente al conflicto religioso.

## Antecedentes

Existen numerosos estudios hist√≥ricos sobre la Guerra Cristera y el proceso de consolidaci√≥n del Estado laico en M√©xico. Sin embargo, son escasos los trabajos que analizan el discurso oficial de este per√≠odo mediante herramientas de miner√≠a de texto y procesamiento de lenguaje natural.

Desde el enfoque hist√≥rico-pol√≠tico, @velasco2015estado analiza la consolidaci√≥n del Estado laico como estrategia de afirmaci√≥n frente al poder eclesi√°stico en el contexto posrevolucionario. Por su parte, De la @fuente1997clericalismo ofrece una reconstrucci√≥n del conflicto entre religi√≥n y pol√≠tica desde la independencia hasta la d√©cada del treinta, enmarcando la Ley Calles en un proceso secularizador m√°s amplio.

En cuanto a estudios centrados en los a√±os veinte, @garciaugarte1997catolicos examina c√≥mo el gobierno construy√≥ discursivamente a los cat√≥licos como actores pol√≠ticos desestabilizadores. A su vez, @villanueva2015intentos demuestra que el discurso estatal contribuy√≥ a la prolongaci√≥n del conflicto y al fracaso de las negociaciones de paz. Por otra parte, @buchenau2007plutarco brinda una lectura integral de la trayectoria ideol√≥gica de Plutarco El√≠as Calles, contextualizando el marco discursivo de los documentos analizados. Desde una perspectiva metodol√≥gica, @grimmer2013text ofrecen un marco cr√≠tico sobre las posibilidades y limitaciones del an√°lisis automatizado de textos pol√≠ticos. En esa misma l√≠nea, @bechmann2019unsupervised abordan las implicancias epistemol√≥gicas de los algoritmos en la producci√≥n de conocimiento textual. En el plano t√©cnico, el manual de @silge2017text brinda herramientas operativas para el procesamiento y visualizaci√≥n de datos textuales en R. Por otra parte, @becerra2021topic reflexionan sobre el v√≠nculo entre modelado de t√≥picos y an√°lisis cualitativo, destacando la relevancia de articular enfoques computacionales e interpretativos en el campo de las ciencias sociales.

En conjunto, estos antecedentes ofrecen un marco te√≥rico y metodol√≥gico robusto que sustenta el abordaje exploratorio de los discursos oficiales, orientado a comprender las representaciones estatales del conflicto religioso. Asimismo, permiten situar esta investigaci√≥n en la intersecci√≥n entre el an√°lisis del discurso, la sociolog√≠a y las t√©cnicas computacionales aplicadas al estudio de textos hist√≥ricos.

## Desarrollo

```{r,include=FALSE}
# =============================================================================
# 1. CARGA Y LIMPIEZA EXHAUSTIVA DE DATOS CON VALIDACI√ìN
# =============================================================================

# Funci√≥n de logging mejorada
log_proceso <- function(mensaje, nivel = "INFO") {
  timestamp <- format(Sys.time(), "%Y-%m-%d %H:%M:%S")
  cat(sprintf("[%s] %s: %s\n", timestamp, nivel, mensaje))
}

log_proceso("Iniciando an√°lisis robusto de texto")

# Cargar el √≠ndice con validaci√≥n
tryCatch({
  docs_index <- read_csv("../../Data/Data/db/Originales/index.csv")
  log_proceso(sprintf("Cargados %d documentos del √≠ndice", nrow(docs_index)))
}, error = function(e) {
  log_proceso(paste("Error al cargar √≠ndice:", e$message), "ERROR")
})

# Funci√≥n avanzada de limpieza de texto con m√∫ltiples etapas
limpiar_texto_avanzado <- function(texto) {
  if(is.na(texto)) return(NA_character_)
  
  texto %>%
    # Etapa 1: Normalizaci√≥n inicial
    str_to_lower() %>%
    # Reemplazar caracteres unicode problem√°ticos
    textclean::replace_non_ascii() %>%
    # Normalizar espacios en blanco especiales
    str_replace_all("[\u00A0\u2007\u202F]", " ") %>%
    # Etapa 2: Limpieza de elementos estructurales
    str_replace_all("\\b(p√°gina|p√°g|cap|cap√≠tulo|fig|figura)\\s*\\d+\\b", "") %>%
    str_replace_all("\\b(art|art√≠culo)\\s*\\d+", "") %>%
    str_replace_all("\\([^)]{0,3}\\)", "") %>%  # Par√©ntesis con poco contenido
    # Etapa 3: Limpieza de caracteres especiales
    str_replace_all("[^\\p{L}\\s\\.,;:!?¬°¬ø]", " ") %>%
    # Etapa 4: Normalizaci√≥n de espacios
    str_replace_all("\\s+", " ") %>%
    str_trim() %>%
    # Etapa 5: Filtros de calidad
    {ifelse(str_length(.) < 20, NA_character_, .)} %>%
    # Etapa 6: Normalizaci√≥n de caracteres
    stri_trans_general("Latin-ASCII")
}

# Funci√≥n de validaci√≥n de calidad de texto
validar_calidad_texto <- function(texto) {
  if (is.na(texto)) return(FALSE)
  
  longitud_ok <- str_length(texto) >= 20
  palabras_ok <- str_count(texto, "\\S+") >= 5
  no_repetitivo <- !str_detect(texto, "(\\b\\w+\\b)(?:\\s+\\1){3,}")
  
  # Solo caracteres imprimibles (letras, puntuaci√≥n, espacios, etc.)
  charset_ok <- stri_detect_regex(texto, "^[\\p{L}\\p{N}\\p{P}\\p{Zs}]+$")
  
  return(longitud_ok & palabras_ok & no_repetitivo & charset_ok)
}


# Funci√≥n segura mejorada con reintentos
safe_read_file_mejorado <- function(path, encoding = "UTF-8", max_intentos = 3) {
  for(intento in 1:max_intentos) {
    tryCatch({
      return(read_file(path, locale = locale(encoding = encoding)))
    }, error = function(e) {
      if(intento < max_intentos) {
        # Intentar con diferentes encodings
        encodings <- c("latin1", "windows-1252", "ISO-8859-1")
        if(intento <= length(encodings)) {
          tryCatch({
            return(read_file(path, locale = locale(encoding = encodings[intento])))
          }, error = function(e2) {
            log_proceso(sprintf("Intento %d fallido para %s: %s", intento, path, e2$message), "WARN")
          })
        }
      } else {
        log_proceso(sprintf("Archivo no legible despu√©s de %d intentos: %s", max_intentos, path), "ERROR")
        return(NA_character_)
      }
    })
  }
  return(NA_character_)
}

# Procesamiento mejorado con m√©tricas de calidad
log_proceso("Iniciando limpieza y validaci√≥n de textos")

docs_index <- docs_index %>%
  mutate(
    # Leer texto con funci√≥n mejorada
    texto_original = map_chr(ubicacion, safe_read_file_mejorado),
    # Aplicar limpieza avanzada
    texto_limpio = map_chr(texto_original, limpiar_texto_avanzado),
    # M√©tricas de calidad
    longitud_original = str_length(texto_original),
    longitud_limpia = str_length(texto_limpio),
    num_palabras_original = str_count(texto_original, "\\S+"),
    num_palabras_limpia = str_count(texto_limpio, "\\S+"),
    num_oraciones = str_count(texto_limpio, "[.!?]+"),
    densidad_puntuacion = str_count(texto_limpio, "[.,;:!?]") / str_length(texto_limpio),
    # Validaci√≥n de calidad
    calidad_ok = map_lgl(texto_limpio, validar_calidad_texto),
    # Detecci√≥n de idioma (simple)
    posible_espa√±ol = str_detect(texto_limpio, "\\b(el|la|de|que|y|en|un|es|se|no|te|lo|le|da|su|por|son|con|para|al|del|los|las|una)\\b"),
    # M√©tricas adicionales
    ratio_reduccion = (longitud_original - longitud_limpia) / longitud_original,
    complejidad_lexica = num_palabras_limpia / num_oraciones
  ) %>%
  # Filtrar documentos v√°lidos
  filter(calidad_ok & posible_espa√±ol & !is.na(texto_limpio)) %>%
  # Renombrar para compatibilidad
  rename(texto = texto_limpio)

# An√°lisis de calidad de datos mejorado
analisis_calidad <- list(
  docs_cargados = nrow(docs_index),
  docs_validos = sum(docs_index$calidad_ok, na.rm = TRUE),
  promedio_palabras = round(mean(docs_index$num_palabras_limpia, na.rm = TRUE)),
  promedio_oraciones = round(mean(docs_index$num_oraciones, na.rm = TRUE)),
  reduccion_promedio = round(mean(docs_index$ratio_reduccion, na.rm = TRUE) * 100, 1),
  complejidad_promedio = round(mean(docs_index$complejidad_lexica, na.rm = TRUE), 1)
)

# Tabla de calidad con reactable mejorada
tabla_calidad <- tibble(
  M√©trica = c("Documentos cargados", "Documentos v√°lidos", "Tasa de √©xito (%)",
              "Promedio palabras/doc", "Promedio oraciones/doc", "Complejidad l√©xica promedio",
              "Reducci√≥n de texto (%)", "Documentos en espa√±ol (%)"),
  Valor = c(analisis_calidad$docs_cargados,
            analisis_calidad$docs_validos,
            round(analisis_calidad$docs_validos/analisis_calidad$docs_cargados*100, 1),
            analisis_calidad$promedio_palabras,
            analisis_calidad$promedio_oraciones,
            analisis_calidad$complejidad_promedio,
            analisis_calidad$reduccion_promedio,
            round(sum(docs_index$posible_espa√±ol, na.rm = TRUE)/nrow(docs_index)*100, 1))
)
```

### Caracterizaci√≥n de los datos

Los documentos, a examinar est√°n relacionados de la siguiente forma, 4 columnas que son como se muestran en @tbl-documentos:

-   documento: hace relacion al nombre del documento a analizar
-   tipo: habla del tipo de docuento analizado
-   anio: A√±o en el el documento fue emitido
-   texto: (texto_corto) Hace referencia al contenido del texto a analizar

```{r, include=FALSE}

documentos <- reactable(
  tabla_calidad,
  striped = TRUE,
  highlight = TRUE,
  bordered = TRUE,
  theme = reactableTheme(
    headerStyle = list(backgroundColor = "#f8f9fa", fontWeight = "bold"),
    cellPadding = "8px"
  ),
  columns = list(
    M√©trica = colDef(minWidth = 200),
    Valor = colDef(align = "center", style = list(fontWeight = "bold"))
  )
)

```

Tal como se puede apreciar en la tabla, se han an√°lisis ,  cargado,  y procesado 10 documentos , con una tasa de √©xito de cargue del 100%. A su vez el promedio de palabras obtenidas por documento es de 1.424 palabras y 37 oraciones por p√°rrafo, en un instante inicial se pudo detectar que como lo muestra la tabla, los 10 textos tienen una complejidad l√©xica de 59.8%, mientras que la reducci√≥n de los textos en su conjunto fue de 0.9%.


```{r}
documentos
```

### Tokenizaci√≥n de datos

A partir de la columna texto, se realiza una tokenizaci√≥n como se muestra en la tabla @tbl-frecuencias

```{r, include=FALSE}
# =============================================================================
# 2. TOKENIZACI√ìN AVANZADA CON VALIDACI√ìN LING√ú√çSTICA
# =============================================================================

# Stopwords expandidas con contexto hist√≥rico y regional
stopwords_contextuales <- c(
  stopwords("es"),
  # Formas hist√≥ricas y arcaicas
  c("deste", "desa", "aqueste", "aquesa", "donde", "quando", "quien", "qual",
    "se√±or", "se√±ora", "don", "do√±a", "vuestra", "vuestro", "merced"),
  # Conectores y muletillas del per√≠odo
  c("pues", "luego", "empero", "mas", "sino", "aunque", "porque", "puesto",
    "asi", "asimismo", "tambien", "aun", "ademÔøΩs", "incluso"),
  # T√©rminos administrativos comunes
  c("articulo", "capitulo", "seccion", "parte", "titulo", "numero", "pagina",
    "folio", "documento", "expediente", "archivo"),
  # N√∫meros en texto
  c("uno", "dos", "tres", "cuatro", "cinco", "seis", "siete", "ocho", "nueve", "diez",
    "primer", "segundo", "tercero", "cuarto", "quinto"),
  # Palabras muy comunes sin valor sem√°ntico
  c("cosa", "cosas", "forma", "manera", "modo", "caso", "casos", "vez", "veces",
    "dia", "dias", "a√±o", "a√±os", "tiempo", "momento", "parte", "partes")
)
```



```{r, include=FALSE}
# Funci√≥n de tokenizaci√≥n mejorada con validaci√≥n
tokenizar_avanzado <- function(df, columna_texto) {
  log_proceso("Iniciando tokenizaci√≥n avanzada")
  
  tokens <- df %>%
    # Tokenizaci√≥n b√°sica
    unnest_tokens(word, {{columna_texto}}) %>%
    # Filtro de stopwords expandido
    filter(!word %in% stopwords_contextuales) %>%
    # Filtros de calidad mejorados
    filter(
      # Longitud apropiada
      str_length(word) >= 3 & str_length(word) <= 20,
      # No solo n√∫meros
      !str_detect(word, "^\\d+$"),
      # No solo caracteres repetidos
      !str_detect(word, "^(.)\\1{2,}$"),
      # No fragmentos HTML/XML
      !str_detect(word, "^(lt|gt|amp|nbsp|quot)$"),
      # Caracteres alfab√©ticos v√°lidos
      str_detect(word, "^[a-z√°√©√≠√≥√∫√±√º]+$")
    ) %>%
    # Stemming contextual
    mutate(
      word_original = word,
      word = wordStem(word, language = "spanish")
    ) %>%
    # Filtrar stems muy cortos resultantes
    filter(str_length(word) >= 2)
  
  log_proceso(sprintf("Tokenizaci√≥n completada: %d tokens √∫nicos", n_distinct(tokens$word)))
  return(tokens)
}

# Aplicar tokenizaci√≥n
tokens <- tokenizar_avanzado(docs_index, texto)
```

```{r}
#| label: tbl-frecuencias
#| tbl-cap: "Top 20 palabras m√°s frecuentes (sin stopwords ni n√∫meros)"

# An√°lisis de tokenizaci√≥n
token_stats <- tokens %>%
  summarise(
    total_tokens = n(),
    tokens_unicos = n_distinct(word),
    ratio_diversidad = tokens_unicos / total_tokens,
    palabras_por_doc = total_tokens / n_distinct(anio),
    .groups = "drop"
  ) %>%
  reactable()
token_stats
```
La anterior tabla, nos muestra que el total de tokens recopilado en los 10 documentos fue de 6.768, con un total de 1.701 tokens √∫nicos, que proporcionen una diversidad entre los mismos de 25.13%, obteniendo un total promedio d epalabras por documentos de 2.256.

### Frecuencia de palabras por a√±o

```{r, include=FALSE}
# =============================================================================
# 3. AN√ÅLISIS DE FRECUENCIAS CON VALIDACI√ìN ESTAD√çSTICA
# =============================================================================

# Frecuencias con estad√≠sticas robustas
freq_detallada <- tokens %>%
  count(word, sort = TRUE) %>%
  mutate(
    porcentaje = n / sum(n) * 100,
    acumulado = cumsum(porcentaje),
    rango = row_number(),
    # Clasificaci√≥n por frecuencia
    categoria_freq = case_when(
      n >= quantile(n, 0.95) ~ "Muy Alta",
      n >= quantile(n, 0.75) ~ "Alta", 
      n >= quantile(n, 0.50) ~ "Media",
      n >= quantile(n, 0.25) ~ "Baja",
      TRUE ~ "Muy Baja"
    ),
    # M√©tricas de distribuci√≥n
    z_score = scale(n)[,1],
    percentil = percent_rank(n)
  )

# An√°lisis de Zipf mejorado con ajuste estad√≠stico
analisis_zipf <- freq_detallada %>%
  filter(rango <= 1000) %>%
  mutate(
    log_rango = log10(rango),
    log_freq = log10(n)
  )

# Ajuste del modelo de Zipf
modelo_zipf <- lm(log_freq ~ log_rango, data = analisis_zipf)
zipf_summary <- broom::tidy(modelo_zipf)

# Visualizaci√≥n de Zipf mejorada
p_zipf <- analisis_zipf %>%
  ggplot(aes(x = log_rango, y = log_freq)) +
  geom_point(alpha = 0.6, color = "steelblue", size = 1.5) +
  geom_smooth(method = "lm", se = TRUE, color = "red", linetype = "dashed") +
  labs(
    title = "Distribuci√≥n de Zipf - An√°lisis de Ley de Potencias",
    subtitle = sprintf("R¬≤ = %.3f, Pendiente = %.3f (p < %.3f)", 
                       summary(modelo_zipf)$r.squared, 
                       zipf_summary$estimate[2], 
                       zipf_summary$p.value[2]),
    x = "Log‚ÇÅ‚ÇÄ(Rango)", 
    y = "Log‚ÇÅ‚ÇÄ(Frecuencia)",
    caption = "Una pendiente cercana a -1 indica cumplimiento de la Ley de Zipf"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 12, color = "gray60")
  )

p_zipf
```

```{r}
p_zipf
```
La distribuci√≥n l√©xica del corpus analizado presenta un ajuste robusto a una ley de potencias del tipo Zipf, con un coeficiente de determinaci√≥n R¬≤ = 0.947 y una pendiente estimada de -0.772 (p < 0.000). Aunque la pendiente se desv√≠a ligeramente del valor te√≥rico de -1, la relaci√≥n sigue siendo significativa y evidencia una fuerte estructura jer√°rquica en la frecuencia de palabras, caracter√≠stica de los sistemas ling√º√≠sticos naturales, por tanto los documentos analizados arrojaran en ese mismo sentido una rica variedad interpretativa ajustada a los modelos estad√≠sticos estudiados.

```{r, include=FALSE}
# =============================================================================
# 4. AN√ÅLISIS TEMPORAL CON PRUEBAS ESTAD√çSTICAS
# =============================================================================

# Keywords expandidas y contextualizadas hist√≥ricamente
keywords_historicas <- tribble(
  ~palabra, ~categoria, ~subcategoria, ~peso,

  # Religi√≥n cat√≥lica
  "religion", "Religioso", "Doctrina", 3,
  "catolico", "Religioso", "Identidad", 3,
  "iglesia", "Religioso", "Instituci√≥n", 3,
  "fe", "Religioso", "Creencia", 2,
  "culto", "Religioso", "Pr√°ctica", 2,
  "sagrado", "Religioso", "Sacralidad", 2,
  "divino", "Religioso", "Divinidad", 2,
  "bendicion", "Religioso", "Ritual", 1,
  "sacerdotes", "Religioso", "Actor", 2,
  "clero", "Religioso", "Instituci√≥n", 2,

  # Autoridad y orden social
  "orden", "Autoridad", "Estructura", 3,
  "autoridad", "Autoridad", "Poder", 3,
  "obediencia", "Autoridad", "Sumisi√≥n", 2,
  "disciplina", "Autoridad", "Control", 2,
  "jerarquia", "Autoridad", "Estratificaci√≥n", 2,
  "gobierno", "Autoridad", "Pol√≠tica", 2,
  "ley", "Autoridad", "Legal", 2,
  "legalidad", "Autoridad", "Legal", 2,
  "Constituci√≥n", "Autoridad", "Legal", 3,
  "soberan√≠a", "Autoridad", "Estado", 3,

  # Conflicto y resistencia
  "revolucion", "Conflicto", "Revoluci√≥n", 3,
  "rebelion", "Conflicto", "Resistencia", 3,
  "guerra", "Conflicto", "Violencia", 3,
  "enemigo", "Conflicto", "Oposici√≥n", 2,
  "amenaza", "Conflicto", "Peligro", 2,
  "lucha", "Conflicto", "Combate", 2,
  "fanatismo", "Conflicto", "Religioso", 2,
  "superstici√≥n", "Conflicto", "Religioso", 2,
  "herej√≠a", "Conflicto", "Religioso", 2,
  "conspiraci√≥n", "Conflicto", "Complot", 2,
  "subversi√≥n", "Conflicto", "Inestabilidad", 2,

  # Moral y valores
  "moral", "Moral", "√âtica", 3,
  "virtud", "Moral", "Bondad", 2,
  "honor", "Moral", "Dignidad", 2,
  "deber", "Moral", "Obligaci√≥n", 2,
  "justicia", "Moral", "Equidad", 2,
  "pecado", "Moral", "Transgresi√≥n", 2,
  "civilizaci√≥n", "Moral", "Modernidad", 2,
  "progreso", "Moral", "Modernidad", 2,
  "atraso", "Moral", "Tradicionalismo", 2,
  "educaci√≥n laica", "Moral", "Modernidad", 2,

  # Identidad nacional
  "patria", "Nacional", "Territorio", 3,
  "nacion", "Nacional", "Comunidad", 3,
  "mexicano", "Nacional", "Identidad", 2,
  "pueblo", "Nacional", "Ciudadan√≠a", 2,
  "bandera", "Nacional", "S√≠mbolo", 1,
  "ciudadanos", "Nacional", "Ciudadan√≠a", 2,

  # Modernizaci√≥n
  "educacion", "Modernidad", "Instrucci√≥n", 2,
  "ciencia", "Modernidad", "Conocimiento", 2,

  # Violencia represiva
  "castigo", "Represi√≥n", "Sanci√≥n", 2,
  "delito", "Represi√≥n", "Criminalizaci√≥n", 2,
  "infracci√≥n", "Represi√≥n", "Legalidad", 2,
  "represi√≥n", "Represi√≥n", "Coerci√≥n", 3,
  "sanci√≥n", "Represi√≥n", "Justificaci√≥n", 2
)


# An√°lisis temporal robusto con ventanas m√≥viles
analisis_temporal_robusto <- tokens %>%
  inner_join(keywords_historicas, by = c("word" = "palabra")) %>%
  group_by(anio, categoria, subcategoria) %>%
  summarise(
    frecuencia = n(),
    frecuencia_ponderada = sum(peso),
    .groups = "drop"
  ) %>%
  # Calcular totales por a√±o
  group_by(anio) %>%
  mutate(
    total_anio = sum(frecuencia),
    total_ponderado_anio = sum(frecuencia_ponderada),
    porcentaje = frecuencia / total_anio * 100,
    porcentaje_ponderado = frecuencia_ponderada / total_ponderado_anio * 100
  ) %>%
  ungroup() %>%
  # Calcular tendencias
  group_by(categoria) %>%
  mutate(
    tendencia = porcentaje_ponderado - lag(porcentaje_ponderado, default = first(porcentaje_ponderado)),
    cambio_absoluto = abs(tendencia)
  ) %>%
  ungroup()

# Prueba estad√≠stica para cambios significativos
if(length(unique(analisis_temporal_robusto$anio)) > 1) {
  # ANOVA para diferencias entre a√±os
  modelo_temporal <- aov(porcentaje_ponderado ~ factor(anio) + categoria, 
                         data = analisis_temporal_robusto)
  anova_summary <- broom::tidy(modelo_temporal)
  
  log_proceso(sprintf("ANOVA temporal - F(a√±o): %.3f, p-valor: %.4f", 
                      anova_summary$statistic[1], anova_summary$p.value[1]))
}

# Visualizaci√≥n temporal mejorada con facetas
p_temporal_mejorado <- analisis_temporal_robusto %>%
  ggplot(aes(x = anio, y = porcentaje_ponderado, color = categoria)) +
  geom_line(size = 1.2, alpha = 0.8) +
  geom_point(size = 3, alpha = 0.9) +
  geom_smooth(method = "loess", se = FALSE, linetype = "dashed", alpha = 0.5) +
  facet_wrap(~categoria, scales = "free_y", ncol = 3) +
  labs(
    title = "Evoluci√≥n Temporal de Categor√≠as Tem√°ticas (1926-1928)",
    subtitle = "An√°lisis ponderado por relevancia hist√≥rica con tendencias suavizadas",
    x = "A√±o", 
    y = "Porcentaje Ponderado (%)",
    color = "Categor√≠a",
    caption = "L√≠neas punteadas muestran tendencias suavizadas (LOESS)"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    strip.text = element_text(face = "bold"),
    plot.title = element_text(size = 14, face = "bold")
  ) +
  scale_color_manual(values = wesanderson::wes_palette("Darjeeling1", 
                                                       n = length(unique(analisis_temporal_robusto$categoria)), 
                                                       type = "continuous"))

plotly::ggplotly(p_temporal_mejorado) %>%
  layout(height = 600)
```

```{r}
#| warning: false
#| message: false
#| echo: false
plotly::ggplotly(p_temporal_mejorado) %>%
  layout(height = 600)
```
En el anterior gr√°fico, de Evoluci√≥n temporal de categor√≠as tem√°ticas comprendidas desde 1926 hasta 1928 nos muestra que se encontraron 6 temas de inter√©s, donde la "autoridad" tuvo variaciones c√≠clicas con una tendencia negativa en el tiempo, mientras que el tema "conflicto" tuvo una tendencia creciente en el tiempo estudiado,  al igual de lo obtenido en el tema de lo "nacional", por otra parte el tema de la "moral" desde el 1926 hasta el 1927 fue un tema creciente, pero desde 1927 hasta 1928 su tendencia fue decreciente.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# =============================================================================
# 5. AN√ÅLISIS DE N-GRAMAS CON SIGNIFICANCIA ESTAD√çSTICA
# =============================================================================
# Funci√≥n mejorada para n-gramas con validaci√≥n
extraer_ngramas_avanzado <- function(df, n = 2, min_freq = 2, filtro_calidad = TRUE) {
  
  log_proceso(sprintf("Extrayendo %d-gramas con frecuencia m√≠nima %d", n, min_freq))
  
  ngramas <- df %>%
    unnest_tokens(ngram, texto, token = "ngrams", n = n) %>%
    separate(ngram, paste0("word", 1:n), sep = " ", remove = FALSE) %>%
    # Filtrar stopwords en cada posici√≥n
    filter_at(vars(starts_with("word")), all_vars(!. %in% stopwords_contextuales)) %>%
    # Filtros de calidad si se solicita
    {if(filtro_calidad) {
      filter_at(., vars(starts_with("word")), all_vars(
        !str_detect(., "\\d+") & 
          str_length(.) > 2 & 
          str_detect(., "^[a-z√°√©√≠√≥√∫√±√º]+$")
      ))
    } else .} %>%
    # Recomponer n-grama limpio
    unite(ngram_limpio, starts_with("word"), sep = " ") %>%
    count(ngram_limpio, sort = TRUE) %>%
    filter(n >= min_freq) %>%
    # Calcular m√©tricas adicionales
    mutate(
      longitud_promedio = map_dbl(str_split(ngram_limpio, " "), ~mean(str_length(.x))),
      coherencia = n / sum(n) * 100,
      rango = row_number()
    )
  
  log_proceso(sprintf("Extra√≠dos %d %d-gramas √∫nicos", nrow(ngramas), n))
  return(ngramas)
}

# Funci√≥n para categorizar bigramas (personal√≠zala seg√∫n tu contexto)
categorizar_bigramas <- function(palabra1, palabra2) {
  # Aqu√≠ defines las reglas de categorizaci√≥n seg√∫n tu an√°lisis
  case_when(
    str_detect(paste(palabra1, palabra2), "fanatismo|religioso|iglesia|clero") ~ "religi√≥n como amenaza",
    str_detect(paste(palabra1, palabra2), "enemigo|com√∫n|interferencia") ~ "religi√≥n como enemiga del Estado",
    str_detect(paste(palabra1, palabra2), "poder|civil|orden|p√∫blico") ~ "legitimaci√≥n del poder estatal",
    str_detect(paste(palabra1, palabra2), "castigo|ejemplar|violencia") ~ "violencia leg√≠tima",
    str_detect(paste(palabra1, palabra2), "ley|obediencia|c√≠vica") ~ "obediencia a la ley",
    str_detect(paste(palabra1, palabra2), "progreso|moral|civilizaci√≥n") ~ "proyecto civilizatorio",
    str_detect(paste(palabra1, palabra2), "disciplina|sujeto|control") ~ "sujeto disciplinado",
    TRUE ~ "otras tem√°ticas"
  )
}

# Extraer diferentes tipos de n-gramas
bigramas_avanzados <- extraer_ngramas_avanzado(docs_index, n = 2, min_freq = 3)
trigramas_avanzados <- extraer_ngramas_avanzado(docs_index, n = 3, min_freq = 2)

# An√°lisis de colocaciones (palabras que aparecen juntas m√°s de lo esperado)
colocaciones <- tokens %>%
  pairwise_count(word, ubicacion, sort = TRUE) %>%
  filter(n >= 3) %>%
  mutate(
    # Calcular probabilidades esperadas vs observadas
    prob_observada = n / sum(n),
    significancia = log2(prob_observada / (0.001))  # Simplificado
  ) %>%
  filter(significancia > 0) %>%
  arrange(desc(significancia))

# Preparar datos para la tabla
tabla_bigramas <- bigramas_avanzados %>%
  head(15) %>%
  separate(ngram_limpio, c("palabra_1", "palabra_2"), sep = " ") %>%
  filter(!is.na(palabra_1) & !is.na(palabra_2)) %>%
  mutate(
    frecuencia = n,
    categoria_interpretativa = categorizar_bigramas(palabra_1, palabra_2)
  ) %>%
  select(palabra_1, palabra_2, frecuencia, categoria_interpretativa) %>%
  arrange(desc(frecuencia))

# Funci√≥n para crear barras de progreso personalizadas (despu√©s de crear tabla_bigramas)
crear_barra_progreso <- function(value, max_value = max(tabla_bigramas$frecuencia, na.rm = TRUE)) {
  width <- paste0(round(value / max_value * 100), "%")
  bar <- div(
    style = list(
      background = "#e1e5e9",
      width = "100%",
      height = "16px",
      borderRadius = "3px",
      display = "inline-block"
    ),
    div(
      style = list(
        background = "#3498db",
        width = width,
        height = "100%",
        borderRadius = "3px",
        transition = "width 0.6s ease"
      )
    )
  )
  div(style = list(display = "flex", alignItems = "center", justifyContent = "space-between"),
      bar,
      div(style = list(marginLeft = "8px", fontWeight = "bold"), value)
  )
}

# Crear tabla reactable
tabla_bigramas_reactable <- reactable(
  tabla_bigramas,
  columns = list(
    palabra_1 = colDef(
      name = "Palabra 1",
      minWidth = 120,
      style = list(fontWeight = "bold", color = "#2c3e50")
    ),
    palabra_2 = colDef(
      name = "Palabra 2", 
      minWidth = 120,
      style = list(fontWeight = "bold", color = "#2c3e50")
    ),
    frecuencia = colDef(
      name = "Frecuencia",
      minWidth = 150,
      cell = function(value) {
        # Crear barra de progreso personalizada
        crear_barra_progreso(value)
      },
      align = "left"
    ),
    categoria_interpretativa = colDef(
      name = "Categor√≠a Interpretativa",
      minWidth = 200,
      cell = function(value) {
        # Colores seg√∫n categor√≠a
        color <- case_when(
          value == "religi√≥n como amenaza" ~ "#e74c3c",
          value == "religi√≥n como enemiga del Estado" ~ "#c0392b",
          value == "legitimaci√≥n del poder estatal" ~ "#2980b9",
          value == "violencia leg√≠tima" ~ "#8e44ad",
          value == "obediencia a la ley" ~ "#27ae60",
          value == "proyecto civilizatorio" ~ "#f39c12",
          value == "sujeto disciplinado" ~ "#16a085",
          TRUE ~ "#7f8c8d"
        )
        div(style = list(
          padding = "4px 8px",
          borderRadius = "4px",
          backgroundColor = paste0(color, "20"),
          color = color,
          fontWeight = "500"
        ), value)
      }
    )
  ),
  defaultSorted = "frecuencia",
  defaultSortOrder = "desc",
  striped = TRUE,
  highlight = TRUE,
  bordered = TRUE,
  theme = reactableTheme(
    headerStyle = list(
      backgroundColor = "#34495e",
      color = "white",
      fontWeight = "bold"
    ),
    rowSelectedStyle = list(backgroundColor = "#eee", boxShadow = "inset 2px 0 0 0 #ffa62d")
  ),
  defaultPageSize = 15,
  showPageSizeOptions = TRUE,
  pageSizeOptions = c(10, 15, 20, 25),
  searchable = TRUE,
  resizable = TRUE,
  wrap = FALSE,
  class = "my-table"
)

# Visualizaci√≥n de bigramas con red
if(nrow(bigramas_avanzados) > 10) {
  # Preparar datos para red
  bigramas_red <- bigramas_avanzados %>%
    head(20) %>%
    separate(ngram_limpio, c("word1", "word2"), sep = " ") %>%
    filter(!is.na(word1) & !is.na(word2))
  
  # Crear grafo
  grafo_bigramas <- bigramas_red %>%
    graph_from_data_frame()
  
  # Visualizaci√≥n de red
  p_red_bigramas <- ggraph(grafo_bigramas, layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, 
                   arrow = arrow(type = "closed", length = unit(0.1, "inches"))) +
    geom_node_point(color = "lightblue", size = 3) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1, size = 3) +
    labs(
      title = "Red de Bigramas M√°s Frecuentes",
      subtitle = "Conexiones representan co-ocurrencia de palabras",
      caption = "Grosor de l√≠neas proporcional a frecuencia"
    ) +
    theme_void() +
    theme(
      plot.title = element_text(size = 14, face = "bold"),
      plot.subtitle = element_text(size = 12, color = "gray60")
    )
  
  print(p_red_bigramas)
}
```
En la actual red de Bi-Gramas podemos observar que, como lo explicamos en el gr√°fico, entre m√°s oscuro sea el grosor del vector, la frecuencia de las palabras aumenta, lo que quiere decir que entre m√°s oscura la flecha mas presente ese bi-grama en la composici√≥n de los textos. 

Dado lo anterior Tre-gramas como culto --> religioso y culto --> p√∫blico tienen mucha frecuencia, lo cual quier decir que se repiten seguidamente en los textos analizados,  tambi√©n segunda --> clase, y segundo --> articulo, pero tambi√©n cabe anotar que arresto --> mayor y autoridad --> municipal est√°n muy presente en los textos analizados, lo cual indica el car√°cter autoritario de los mismos.

El an√°lisis de la red de bi-gramas revela patrones l√©xicos significativos en los textos examinados. La intensidad del color en los vectores indica mayor frecuencia, lo que permite identificar combinaciones de palabras recurrentes. Expresiones como culto religioso, culto p√∫blico, segunda clase, segundo art√≠culo, arresto mayor y autoridad municipal destacan por su alta presencia, lo que sugiere no solo una fuerte carga tem√°tica en torno a lo religioso y lo institucional, sino tambi√©n un tono marcadamente autoritario en el discurso analizado.

```{r,include=FALSE}
# =============================================================================
# AN√ÅLISIS DE BIGRAMAS CON CATEGORIZACI√ìN INTERPRETATIVA
# =============================================================================

tabla_bigramas_reales <- bigramas_avanzados %>%
  head(15) %>%  # Tomar los 15 m√°s frecuentes
  separate(ngram_limpio, c("palabra_1", "palabra_2"), sep = " ") %>%
  filter(!is.na(palabra_1) & !is.na(palabra_2)) %>%
  mutate(
    frecuencia = n,
    categoria_interpretativa = categorizar_bigramas(palabra_1, palabra_2)
  ) %>%
  select(palabra_1, palabra_2, frecuencia, categoria_interpretativa) %>%
  arrange(desc(frecuencia))

# Funci√≥n mejorada para categorizar bigramas seg√∫n tu contexto espec√≠fico
categorizar_bigramas_contextual <- function(palabra1, palabra2) {
  bigrama_completo <- paste(palabra1, palabra2, sep = " ")
  
  # Patrones espec√≠ficos para tu an√°lisis hist√≥rico-pol√≠tico
  case_when(
    # Religi√≥n como amenaza
    str_detect(bigrama_completo, "fanatismo.*(religioso|pol√≠tico)|superstici√≥n|herej√≠a|dogma.*peligroso") ~ "religi√≥n como amenaza",
    
    # Religi√≥n vs Estado
    str_detect(bigrama_completo, "enemigo.*(com√∫n|p√∫blico)|interferencia.*(iglesia|clerical)|clero.*(enemigo|opositor)") ~ "religi√≥n como enemiga del Estado",
    
    # Legitimaci√≥n del poder estatal
    str_detect(bigrama_completo, "poder.*(civil|temporal|estatal)|soberan√≠a.*(nacional|popular)|autoridad.*(leg√≠tima|civil)") ~ "legitimaci√≥n del poder estatal",
    
    # Orden p√∫blico y control
    str_detect(bigrama_completo, "orden.*(p√∫blico|social|civil)|paz.*(p√∫blica|social)|tranquilidad.*p√∫blica") ~ "orden p√∫blico",
    
    # Violencia leg√≠tima del Estado
    str_detect(bigrama_completo, "castigo.*(ejemplar|p√∫blico)|represi√≥n.*leg√≠tima|fuerza.*p√∫blica|violencia.*necesaria") ~ "violencia leg√≠tima",
    
    # Obediencia y disciplina
    str_detect(bigrama_completo, "obediencia.*(civil|c√≠vica|legal)|disciplina.*social|sumisi√≥n.*ley") ~ "obediencia a la ley",
    
    # Proyecto modernizador/civilizatorio
    str_detect(bigrama_completo, "progreso.*(moral|social|nacional)|civilizaci√≥n|educaci√≥n.*(laica|p√∫blica)|modernidad") ~ "proyecto civilizatorio",
    
    # Control social y sujeto disciplinado
    str_detect(bigrama_completo, "sujeto.*(disciplinado|obediente)|control.*social|vigilancia") ~ "sujeto disciplinado",
    
    # Educaci√≥n y formaci√≥n ciudadana
    str_detect(bigrama_completo, "educaci√≥n.*(moral|c√≠vica)|formaci√≥n.*ciudadana|instrucci√≥n.*p√∫blica") ~ "educaci√≥n ciudadana",
    
    # Instituciones vs Iglesia
    str_detect(bigrama_completo, "estado.*(laico|secular)|instituci√≥n.*civil|separaci√≥n.*(iglesia|estado)") ~ "secularizaci√≥n institucional",
    
    # Patrones adicionales espec√≠ficos de tu corpus
    TRUE ~ "otras tem√°ticas"
  )
}

# Aplicar nueva categorizaci√≥n
tabla_bigramas_reales <- tabla_bigramas_reales %>%
  mutate(categoria_interpretativa = categorizar_bigramas_contextual(palabra_1, palabra_2))

# Funci√≥n para crear barras de progreso HTML mejoradas
crear_barra_progreso_real <- function(value, max_value = max(tabla_bigramas_reales$frecuencia)) {
  porcentaje <- round((value / max_value) * 100)
  
  div(
    style = list(
      display = "flex",
      alignItems = "center",
      width = "100%"
    ),
    div(
      style = list(
        background = "linear-gradient(90deg, #e8f4fd 0%, #d1ecf1 100%)",
        borderRadius = "12px",
        overflow = "hidden",
        width = "75%",
        height = "22px",
        marginRight = "12px",
        border = "1px solid #b0bec5",
        boxShadow = "inset 0 1px 3px rgba(0,0,0,0.1)"
      ),
      div(
        style = list(
          background = "linear-gradient(90deg, #1e88e5 0%, #1565c0 100%)",
          height = "100%",
          width = paste0(porcentaje, "%"),
          transition = "width 1s ease-in-out",
          borderRadius = "12px",
          boxShadow = "0 2px 4px rgba(30, 136, 229, 0.3)"
        )
      )
    ),
    span(
      style = list(
        fontWeight = "bold",
        fontSize = "15px",
        color = "#1565c0",
        minWidth = "35px",
        textAlign = "right"
      ),
      value
    )
  )
}

# Funci√≥n para crear badges de categor√≠as con colores contextuales
crear_badge_categoria_contextual <- function(categoria) {
  colores_categoria <- list(
    "religi√≥n como amenaza" = list(bg = "#ffebee", color = "#c62828", border = "#ef5350"),
    "religi√≥n como enemiga del Estado" = list(bg = "#fce4ec", color = "#ad1457", border = "#e91e63"),
    "legitimaci√≥n del poder estatal" = list(bg = "#e3f2fd", color = "#1565c0", border = "#2196f3"),
    "orden p√∫blico" = list(bg = "#e8eaf6", color = "#3f51b5", border = "#5c6bc0"),
    "violencia leg√≠tima" = list(bg = "#f3e5f5", color = "#6a1b9a", border = "#9c27b0"),
    "obediencia a la ley" = list(bg = "#e8f5e8", color = "#2e7d32", border = "#4caf50"),
    "proyecto civilizatorio" = list(bg = "#fff3e0", color = "#f57c00", border = "#ff9800"),
    "sujeto disciplinado" = list(bg = "#e0f2f1", color = "#00695c", border = "#009688"),
    "educaci√≥n ciudadana" = list(bg = "#f1f8e9", color = "#558b2f", border = "#8bc34a"),
    "secularizaci√≥n institucional" = list(bg = "#e1f5fe", color = "#0277bd", border = "#03a9f4"),
    "otras tem√°ticas" = list(bg = "#f5f5f5", color = "#616161", border = "#9e9e9e")
  )
  
  colores <- colores_categoria[[categoria]] %||% 
    list(bg = "#f5f5f5", color = "#616161", border = "#9e9e9e")
  
  span(
    style = list(
      backgroundColor = colores$bg,
      color = colores$color,
      border = paste0("2px solid ", colores$border),
      padding = "8px 14px",
      borderRadius = "25px",
      fontSize = "13px",
      fontWeight = "700",
      textTransform = "uppercase",
      letterSpacing = "0.8px",
      whiteSpace = "nowrap",
      display = "inline-block",
      boxShadow = paste0("0 2px 4px ", colores$border, "30")
    ),
    categoria
  )
}

# Crear tabla reactable con datos reales
tabla_bigramas_final_real <- reactable(
  tabla_bigramas_reales,
  columns = list(
    palabra_1 = colDef(
      name = "Primera Palabra",
      minWidth = 140,
      style = list(
        fontWeight = "800",
        color = "#1a237e",
        fontSize = "15px",
        textTransform = "capitalize"
      ),
      headerStyle = list(
        backgroundColor = "#283593",
        color = "white",
        fontWeight = "bold",
        textAlign = "center",
        fontSize = "14px"
      )
    ),
    palabra_2 = colDef(
      name = "Segunda Palabra",
      minWidth = 140,
      style = list(
        fontWeight = "800",
        color = "#1a237e",
        fontSize = "15px",
        textTransform = "capitalize"
      ),
      headerStyle = list(
        backgroundColor = "#283593",
        color = "white",
        fontWeight = "bold",
        textAlign = "center",
        fontSize = "14px"
      )
    ),
    frecuencia = colDef(
      name = "Frecuencia",
      minWidth = 200,
      cell = function(value) {
        crear_barra_progreso_real(value)
      },
      headerStyle = list(
        backgroundColor = "#283593",
        color = "white",
        fontWeight = "bold",
        textAlign = "center",
        fontSize = "14px"
      )
    ),
    categoria_interpretativa = colDef(
      name = "Categor√≠a Interpretativa",
      minWidth = 280,
      cell = function(value) {
        crear_badge_categoria_contextual(value)
      },
      headerStyle = list(
        backgroundColor = "#283593",
        color = "white",
        fontWeight = "bold",
        textAlign = "center",
        fontSize = "14px"
      )
    )
  ),
  defaultSorted = "frecuencia",
  defaultSortOrder = "desc",
  striped = TRUE,
  highlight = TRUE,
  bordered = TRUE,
  theme = reactableTheme(
    headerStyle = list(
      backgroundColor = "#283593",
      color = "white",
      fontWeight = "bold",
      fontSize = "14px",
      borderColor = "#1a237e"
    ),
    rowSelectedStyle = list(
      backgroundColor = "#e8eaf6",
      boxShadow = "inset 3px 0 0 0 #283593"
    ),
    rowHighlightStyle = list(
      backgroundColor = "#f3e5f5",
      transform = "scale(1.02)",
      transition = "all 0.2s ease"
    ),
    borderColor = "#e0e0e0",
    stripedColor = "#fafafa"
  ),
  defaultPageSize = 15,
  showPageSizeOptions = TRUE,
  pageSizeOptions = c(10, 15, 20, 25),
  searchable = TRUE,
  resizable = TRUE,
  wrap = FALSE,
  style = list(
    fontFamily = "'Segoe UI', Arial, sans-serif",
    fontSize = "14px"
  ),
  # Informaci√≥n detallada de cada bigrama
  details = function(index) {
    bigrama_data <- tabla_bigramas_reales[index, ]
    
    div(
      style = list(
        padding = "20px",
        backgroundColor = "#f8f9fa",
        border = "2px solid #dee2e6",
        borderRadius = "12px",
        margin = "12px 0",
        boxShadow = "0 4px 6px rgba(0,0,0,0.1)"
      ),
      h4(
        style = list(
          color = "#495057",
          marginBottom = "16px",
          fontSize = "18px",
          fontWeight = "bold"
        ),
        paste("üìä An√°lisis detallado:", bigrama_data$palabra_1, "‚Üí", bigrama_data$palabra_2)
      ),
      div(
        style = list(
          display = "grid",
          gridTemplateColumns = "1fr 1fr",
          gap = "16px",
          marginBottom = "16px"
        ),
        div(
          style = list(
            padding = "12px",
            backgroundColor = "white",
            borderRadius = "8px",
            border = "1px solid #e9ecef"
          ),
          strong("Frecuencia absoluta: "), bigrama_data$frecuencia,
          br(),
          strong("Porcentaje del corpus: "), 
          paste0(round((bigrama_data$frecuencia / sum(tabla_bigramas_reales$frecuencia)) * 100, 2), "%")
        ),
        div(
          style = list(
            padding = "12px",
            backgroundColor = "white",
            borderRadius = "8px",
            border = "1px solid #e9ecef"
          ),
          strong("Categor√≠a: "), bigrama_data$categoria_interpretativa,
          br(),
          strong("Tipo de an√°lisis: "), "Bigrama contextual"
        )
      ),
      p(
        style = list(
          color = "#6c757d", 
          fontSize = "14px", 
          lineHeight = "1.6",
          fontStyle = "italic"
        ),
        "Este bigrama representa un elemento clave en el discurso analizado, 
         contribuyendo al entendimiento de los patrones sem√°nticos y las 
         estrategias discursivas presentes en el corpus."
      )
    )
  }
)


```


```{r}
# Mostrar la tabla
tabla_bigramas_final_real


```

Seg√∫n los Bi-gramas analizados, se detectaron 15,  aplicando los filtros correspondientes, lexicones seg√∫n la √©poca se obtuvieron que de las 15 categor√≠as obtenidas caben en "otras tem√°ticas", lo que indicar√≠a varios factores, primero que el nivel de los filtros aun no es tan granular, o que debemos rechazar la hip√≥tesis nula , en la cual planteamos la presencia de autoritarismos y de sentimientos negativos en el periodo y aceptar la hip√≥tesis alternativa de no presencia de lo anterior, sin embargo aun debemos analizar el apartado de los sentimientos y su contexto analizado.


```{r, include=FALSE}
# =============================================================================
# 6. AN√ÅLISIS DE SENTIMIENTOS CONTEXTUALIZADO - VERSI√ìN MEJORADA
# =============================================================================

# Diccionario de sentimientos contextual e hist√≥rico
sentimientos_historicos <- tribble(
  ~word, ~sentimiento, ~intensidad, ~contexto,

  # Positivos religiosos/tradicionales
  "bendicion", "positivo", 3, "religioso",
  "salvacion", "positivo", 3, "religioso", 
  "gloria", "positivo", 3, "religioso",
  "virtud", "positivo", 2, "moral",
  "honor", "positivo", 2, "moral",
  "paz", "positivo", 2, "social",
  "orden", "positivo", 2, "social",
  "patria", "positivo", 3, "nacional",
  "progreso", "positivo", 2, "modernidad",

  # Negativos - amenazas al orden
  "revolucion", "negativo", 3, "pol√≠tico",
  "rebelion", "negativo", 3, "pol√≠tico",
  "enemigo", "negativo", 2, "conflicto",
  "amenaza", "negativo", 2, "conflicto",
  "guerra", "negativo", 3, "conflicto",
  "pecado", "negativo", 2, "religioso",
  "castigo", "negativo", 2, "moral",
  "destruccion", "negativo", 3, "social",
  "fanatismo", "negativo", 3, "religioso",
  "superstici√≥n", "negativo", 2, "religioso",
  "herej√≠a", "negativo", 3, "religioso",
  "subversi√≥n", "negativo", 3, "pol√≠tico",
  "conspiraci√≥n", "negativo", 3, "conflicto",
  "delito", "negativo", 2, "jur√≠dico",
  "infracci√≥n", "negativo", 2, "jur√≠dico",
  "represi√≥n", "negativo", 3, "pol√≠tico",
  "sanci√≥n", "negativo", 2, "jur√≠dico",
  "atraso", "negativo", 2, "modernidad",

  # Autoridad (neutral-positivo en contexto)
  "autoridad", "autoridad", 2, "pol√≠tico",
  "gobierno", "autoridad", 2, "pol√≠tico",
  "obediencia", "autoridad", 1, "social",
  "disciplina", "autoridad", 1, "social",
  "legalidad", "autoridad", 2, "jur√≠dico",
  "soberan√≠a", "autoridad", 2, "pol√≠tico",
  "Constituci√≥n", "autoridad", 2, "jur√≠dico",

  # Positivos institucionales / racionalistas
  "civilizaci√≥n", "positivo", 2, "modernidad",
  "educaci√≥n laica", "positivo", 2, "modernidad",
  "ciudadanos", "positivo", 2, "nacional",
  "naci√≥n", "positivo", 2, "nacional",

  # Emociones b√°sicas
  "alegria", "positivo", 2, "emocional",
  "tristeza", "negativo", 2, "emocional",
  "miedo", "negativo", 2, "emocional",
  "esperanza", "positivo", 2, "emocional"
)


# An√°lisis de sentimientos por a√±o y contexto
sentimientos_evolucion <- tokens %>%
  inner_join(sentimientos_historicos, by = "word") %>%
  group_by(anio, sentimiento, contexto) %>%
  summarise(
    frecuencia = n(),
    intensidad_promedio = mean(intensidad),
    score_ponderado = sum(intensidad),
    .groups = "drop"
  ) %>%
  # Calcular porcentajes y normalizaciones
  group_by(anio) %>%
  mutate(
    total_anio = sum(frecuencia),
    porcentaje = frecuencia / total_anio * 100,
    score_normalizado = score_ponderado / sum(score_ponderado) * 100
  ) %>%
  ungroup() %>%
  # Calcular √≠ndice de sentimiento compuesto
  group_by(anio) %>%
  mutate(
    indice_sentimiento = case_when(
      sentimiento == "positivo" ~ score_normalizado,
      sentimiento == "negativo" ~ -score_normalizado,
      TRUE ~ 0
    )
  ) %>%
  ungroup()

# Calcular √≠ndice de polarizaci√≥n por a√±o
polarizacion_anual <- sentimientos_evolucion %>%
  group_by(anio) %>%
  summarise(
    indice_polarizacion = sum(abs(indice_sentimiento)),
    balance_emocional = sum(indice_sentimiento),
    diversidad_emocional = n_distinct(contexto),
    .groups = "drop"
  )

# Funci√≥n para crear paleta de colores personalizada
crear_paleta_sentimientos <- function(n_colors) {
  colores_base <- c(
    # Positivos
    "#DC143C", "#32CD32", "#228B22", "#90EE90",
    # Negativos  
    "#DC143C", "#B22222", "#FF6347", "#FFB6C1",
    # Autoridad
    "#4169E1", "#6495ED", "#87CEEB", "#E6E6FA",
    # Neutros
    "#D2691E", "#F4A460", "#DDA0DD", "#98FB98"
  )
  
  if (n_colors <= length(colores_base)) {
    return(colores_base[1:n_colors])
  } else {
    return(colorRampPalette(colores_base)(n_colors))
  }
}

# Visualizaci√≥n mejorada con manejo robusto de errores
crear_viz_sentimientos <- function(datos, interactive = TRUE) {
  
  # Verificar si hay datos
  if (nrow(datos) == 0) {
    warning("No hay datos para visualizar")
    return(NULL)
  }
  
  # Preparar datos con validaciones adicionales
  datos_viz <- datos %>%
    # Filtrar valores v√°lidos
    filter(!is.na(score_normalizado), 
           !is.infinite(score_normalizado),
           !is.na(anio)) %>%
    mutate(
      sentimiento_contexto = paste(sentimiento, "-", contexto),
      # Convertir a√±o a num√©rico y asegurar continuidad
      anio_num = as.numeric(anio),
      # Crear factor ordenado para mejor control
      anio_factor = factor(anio, levels = sort(unique(anio)))
    ) %>%
    # Verificar que tengamos al menos 2 a√±os para evitar problemas de diff()
    filter(length(unique(anio_num)) >= 1)
  
  # Validar datos despu√©s de limpieza
  if (nrow(datos_viz) == 0) {
    warning("No hay datos v√°lidos despu√©s de la limpieza")
    return(NULL)
  }
  
  # N√∫mero de categor√≠as √∫nicas
  n_categorias <- length(unique(datos_viz$sentimiento_contexto))
  
  # Crear escala de a√±os expl√≠cita para evitar problemas de continuidad
  anios_completos <- seq(min(datos_viz$anio_num), max(datos_viz$anio_num), by = 1)
  
  # Crear gr√°fico base con configuraciones m√°s robustas
  p_base <- datos_viz %>%
    ggplot(aes(x = anio_num, y = score_normalizado, fill = sentimiento_contexto)) +
    geom_area(alpha = 0.7, position = "stack") +
    facet_wrap(~sentimiento, scales = "free_y", nrow = 2) +
    # Escala x expl√≠cita para mejor control
    scale_x_continuous(
      breaks = anios_completos,
      labels = as.character(anios_completos),
      expand = c(0.02, 0)
    ) +
    scale_y_continuous(expand = c(0, 0)) +
    labs(
      title = "Evoluci√≥n del Panorama Emocional por Contexto (1926-1928)",
      subtitle = "An√°lisis multidimensional de sentimientos con contexto hist√≥rico",
      x = "A√±o", 
      y = "Score Normalizado (%)",
      fill = "Sentimiento - Contexto",
      caption = "√Åreas apiladas muestran contribuci√≥n relativa de cada contexto"
    ) +
    theme_minimal() +
    theme(
      legend.position = "bottom",
      legend.title = element_text(size = 10),
      legend.text = element_text(size = 8),
      strip.text = element_text(face = "bold"),
      plot.title = element_text(size = 14, face = "bold"),
      plot.subtitle = element_text(size = 12),
      panel.grid.minor.x = element_blank()
    ) +
    scale_fill_manual(values = crear_paleta_sentimientos(n_categorias)) +
    guides(fill = guide_legend(nrow = 3, byrow = TRUE))
  
  # Retornar versi√≥n seg√∫n par√°metro
  if (interactive) {
    # Crear plotly con configuraciones m√°s conservadoras
    tryCatch({
      p_interactive <- plotly::ggplotly(p_base, height = 600) %>%
        plotly::layout(
          legend = list(
            orientation = "h",
            x = 0.1,
            y = -0.2
          ),
          # Configuraciones adicionales para estabilidad
          xaxis = list(fixedrange = FALSE),
          yaxis = list(fixedrange = FALSE)
        )
      return(p_interactive)
    }, warning = function(w) {
      # Suprimir warnings espec√≠ficos de plotly
      if (grepl("ning√∫n argumento finito para min", w$message)) {
        # Continuar sin mostrar el warning
        invokeRestart("muffleWarning")
      }
    }, error = function(e) {
      cat("Error en plotly, devolviendo versi√≥n est√°tica:", e$message, "\n")
      return(p_base)
    })
  } else {
    return(p_base)
  }
}

# Funci√≥n auxiliar para suprimir warnings espec√≠ficos
suppressSpecificWarnings <- function(expr, patterns) {
  withCallingHandlers(
    expr,
    warning = function(w) {
      if (any(sapply(patterns, function(p) grepl(p, w$message, fixed = TRUE)))) {
        invokeRestart("muffleWarning")
      }
    }
  )
}

# Crear visualizaci√≥n con manejo completo de errores y warnings
suppressSpecificWarnings({
  
  tryCatch({
    
    # Verificar que los datos existen
    if (!exists("tokens")) {
      stop("El objeto 'tokens' no existe. Aseg√∫rate de haber ejecutado el c√≥digo de tokenizaci√≥n primero.")
    }
    
    # Verificar calidad de datos antes de procesar
    cat("=== DIAGN√ìSTICO DE DATOS ===\n")
    cat("Filas en sentimientos_evolucion:", nrow(sentimientos_evolucion), "\n")
    cat("A√±os √∫nicos:", paste(sort(unique(sentimientos_evolucion$anio)), collapse = ", "), "\n")
    cat("Sentimientos √∫nicos:", paste(unique(sentimientos_evolucion$sentimiento), collapse = ", "), "\n")
    
    # Crear visualizaci√≥n
    viz_sentimientos <- crear_viz_sentimientos(sentimientos_evolucion, interactive = TRUE)
    
    # Mostrar visualizaci√≥n
    if (!is.null(viz_sentimientos)) {
      print(viz_sentimientos)
      cat("\n‚úì Visualizaci√≥n interactiva creada exitosamente\n")
    } else {
      cat("‚ö† No se pudo crear la visualizaci√≥n interactiva\n")
    }
    
  }, error = function(e) {
    cat("‚ùå Error en la visualizaci√≥n:", e$message, "\n")
    cat("Creando versi√≥n est√°tica como alternativa...\n")
    
    # Crear versi√≥n est√°tica como respaldo
    viz_estatica <- crear_viz_sentimientos(sentimientos_evolucion, interactive = FALSE)
    if (!is.null(viz_estatica)) {
      print(viz_estatica)
      cat("‚úì Visualizaci√≥n est√°tica creada como respaldo\n")
    }
  })
  
}, patterns = c("ning√∫n argumento finito para min", "no finite arguments to min"))
```

```{r}
crear_viz_sentimientos(sentimientos_evolucion, interactive = T)
```
La tabla muestra la evoluci√≥n temporal con el a√±adido que es por contexto de cada una. En ese orden de ideas podemos denotar con el color verde la emoci√≥n positiva con un contexto social, que tuvo una tendencia creciente del 1926 hasta 1927, mientras que desde 1927 hasta 1928 su comportamiento fue decreciente, lo cual apunta a confirmar la hip√≥tesis planteada, la cual pasa de 1926 de 60% a 1927 a 98% y a 1928 a 80% .

Por otro lado el contexto negativo - pol√≠tico, se mantuvo constante con una tendencia creciente desde 1926 hasta 1928 en la guerra decretada a la religi√≥n por plutarco elias calle, pasando de una constante de 1926 de 15% a 1928 a 25%.

La evoluci√≥n temporal de los contextos emocionales confirma la hip√≥tesis inicial: mientras la emoci√≥n positiva con trasfondo social mostr√≥ un ascenso hasta 1927 seguido de un descenso en 1928, el contexto negativo-pol√≠tico mantuvo una tendencia creciente, reflejando el endurecimiento del conflicto religioso impulsado por el gobierno de Plutarco El√≠as Calles.

```{r, include=FALSE}
# =============================================================================
# AN√ÅLISIS ADICIONAL: M√âTRICAS DE SENTIMIENTO
# =============================================================================

# Resumen de m√©tricas por a√±o
metricas_sentimiento <- sentimientos_evolucion %>%
  group_by(anio) %>%
  summarise(
    total_palabras_sentimiento = sum(frecuencia),
    contextos_diversos = n_distinct(contexto),
    sentimientos_diversos = n_distinct(sentimiento),
    intensidad_promedio = mean(intensidad_promedio),
    .groups = "drop"
  ) %>%
  # Calcular tendencias
  mutate(
    cambio_palabras = total_palabras_sentimiento - lag(total_palabras_sentimiento),
    cambio_porcentual = (cambio_palabras / lag(total_palabras_sentimiento)) * 100
  )

# Mostrar m√©tricas
cat("\n=== M√âTRICAS DE SENTIMIENTO POR A√ëO ===\n")
print(metricas_sentimiento)

# An√°lisis de contextos dominantes
contextos_dominantes <- sentimientos_evolucion %>%
  group_by(anio, contexto) %>%
  summarise(
    score_total = sum(score_normalizado),
    .groups = "drop"
  ) %>%
  group_by(anio) %>%
  slice_max(score_total, n = 3) %>%
  arrange(anio, desc(score_total))

cat("\n=== CONTEXTOS DOMINANTES POR A√ëO ===\n")
print(contextos_dominantes)

# Visualizaci√≥n de tendencias de polarizaci√≥n
p_polarizacion <- polarizacion_anual %>%
  ggplot(aes(x = anio)) +
  geom_line(aes(y = indice_polarizacion, color = "Polarizaci√≥n"), size = 1.2) +
  geom_line(aes(y = balance_emocional, color = "Balance Emocional"), size = 1.2) +
  geom_point(aes(y = diversidad_emocional * 10, color = "Diversidad x10"), size = 3) +
  labs(
    title = "√çndices de Polarizaci√≥n y Balance Emocional",
    subtitle = "Evoluci√≥n de la tensi√≥n emocional en el discurso (1926-1928)",
    x = "A√±o",
    y = "√çndice",
    color = "M√©trica"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  scale_color_manual(values = c("Polarizaci√≥n" = "#DC143C", 
                                "Balance Emocional" = "#2E8B57",
                                "Diversidad x10" = "#4169E1"))

p_polarizacion
```

```{r}

plotly::ggplotly(p_polarizacion)
```

El gr√°fico muestra la evoluci√≥n de la polarizaci√≥n y el balance emocional en el periodo estudiado que va desde 1926 hasta 1928, lo que indica que la polarizaci√≥n analizada en los textos es de  100 desde 1926 hasta 1928,  mientras que el balance emocional tuvo una tendencia creciente de 1926 hasta 1927, mientras que desde 1927 hasta 1928 tuvo una tendencia decreciente negativa, obteniendo una diversidad en 1926 de 30%, en 1927 de 10% y en 1928 del 20%. Lo anterior quiere decir que en el periodo estudiado el estado jugo un papel polarizador en el tema estudiado, mostrando un balance emocional creciente en el primer a√±o del estudio, convirtiendo una estructura de decreciente moral del 1927 hasta 1928.

```{r, include=FALSE}
# =============================================================================
# 7. MODELADO DE TEMAS (TOPIC MODELING) CON LDA
# =============================================================================

log_proceso("Iniciando modelado de temas con LDA")

# Preparar matriz documento-t√©rmino
dtm_prep <- tokens %>%
  count(ubicacion, word) %>%
  cast_dtm(ubicacion, word, n)

# Encontrar n√∫mero √≥ptimo de temas
if(require(ldatuning, quietly = TRUE) && nrow(dtm_prep) > 5) {
  
  tryCatch({
    # B√∫squeda de n√∫mero √≥ptimo (rango limitado para eficiencia)
    resultado_tuning <- FindTopicsNumber(
      dtm_prep,
      topics = seq(3, min(8, nrow(dtm_prep)-1), by = 1),
      metrics = c("Griffiths2004", "CaoJuan2009"),
      method = "Gibbs",
      control = list(seed = 123),
      verbose = FALSE
    )
    
    num_topics_optimo <- resultado_tuning$topics[which.max(resultado_tuning$Griffiths2004)]
    log_proceso(sprintf("N√∫mero √≥ptimo de temas: %d", num_topics_optimo))
    
  }, error = function(e) {
    log_proceso("Error en tuning de temas, usando valor por defecto", "WARN")
    num_topics_optimo <- min(5, nrow(dtm_prep)-1)
  })
  
} else {
  num_topics_optimo <- min(5, nrow(dtm_prep)-1)
}

# Ajustar modelo LDA
if(num_topics_optimo >= 2) {
  modelo_lda <- LDA(dtm_prep, k = num_topics_optimo, control = list(seed = 123))
  
  # Extraer temas
  temas_palabras <- tidy(modelo_lda, matrix = "beta") %>%
    group_by(topic) %>%
    top_n(10, beta) %>%
    ungroup() %>%
    arrange(topic, -beta)
  
  # Extraer distribuci√≥n de temas por documento
  temas_documentos <- tidy(modelo_lda, matrix = "gamma") %>%
    spread(topic, gamma) %>%
    left_join(docs_index %>% select(ubicacion, anio), by = c("document" = "ubicacion"))
  
  # Visualizar temas principales
  p_temas <- temas_palabras %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(term, beta, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ paste("Tema", topic), scales = "free") +
    coord_flip() +
    scale_x_reordered() +
    labs(
      title = "Temas Principales Identificados por LDA",
      subtitle = "Top 10 palabras m√°s representativas por tema",
      x = "T√©rminos", y = "Probabilidad Beta"
    ) +
    theme_minimal() +
    theme(strip.text = element_text(face = "bold"))
  
  print(p_temas)
  
  # Evoluci√≥n temporal de temas
  if("anio" %in% colnames(temas_documentos)) {
    evolucion_temas <- temas_documentos %>%
      select(-document) %>%
      gather(tema, probabilidad, -anio) %>%
      group_by(anio, tema) %>%
      summarise(prob_promedio = mean(probabilidad, na.rm = TRUE), .groups = "drop")
    
    p_evolucion_temas <- evolucion_temas %>%
      ggplot(aes(x = anio, y = prob_promedio, color = tema)) +
      geom_line(size = 1.2) +
      geom_point(size = 3) +
      labs(
        title = "Evoluci√≥n Temporal de Temas",
        subtitle = "Probabilidad promedio de cada tema por a√±o",
        x = "A√±o", y = "Probabilidad Promedio",
        color = "Tema"
      ) +
      theme_minimal() +
      scale_color_brewer(type = "qual", palette = "Set2")
    
    plotly::ggplotly(p_evolucion_temas)
  }
}
```

```{r}

plotly::ggplotly(p_temas)
```

```{r}
plotly::ggplotly(p_evolucion_temas)
```
Se han identificado 5 temas principales, como se puede apreciar en el gr√°fico anterior,  donde en el gr√°fico de los "Temas principales identificados LDA" donde en el tema 1 se identifican palabras como gobierno, inter√©s, mexico etc, y as√≠ con el tema 2 al 5

Por otro lado en el gr√°fico titulado como "Evoluci√≥n temporal por temas", podemos apreciar la evoluci√≥n de los mismos donde en el tema uno, tuvo un incremento significativo en el periodo 1926-1927, decreciendo en el periodo 1927-1928. 

Los temas 3,4 y 5 tuvieron una similar evoluci√≥n decreciente desde el periodo 1926-1927 terminando en 1927 los temas 3 y 4, mientras que el tema 2 parte de 1927 hasta 1928 con una tendencia clara ascendente. 


```{r, include=FALSE}
# =============================================================================
# 8. AN√ÅLISIS DE COOCURRENCIA Y REDES SEM√ÅNTICAS
# =============================================================================

log_proceso("Construyendo redes sem√°nticas")

# Umbral din√°mico de frecuencia m√≠nima
n_total <- nrow(tokens)
umbral_minimo <- if (n_total < 100) 1 else if (n_total < 1000) 2 else 3

coocurrencia_avanzada <- tokens %>%
  filter(word %in% freq_detallada$word[freq_detallada$rango <= 100]) %>%
  pairwise_count(word, ubicacion, sort = TRUE) %>%
  filter(n >= umbral_minimo) %>%
  mutate(
    prob_conjunta = n / sum(n),
    pmi = log2(prob_conjunta / 0.01)
  ) %>%
  filter(pmi > 0)

log_proceso(sprintf("Se detectaron %d coocurrencias relevantes con n >= %d", 
                    nrow(coocurrencia_avanzada), umbral_minimo))

# Construcci√≥n de grafo si hay al menos 2 conexiones
if(nrow(coocurrencia_avanzada) >= 2) {
  
  n_max <- min(50, nrow(coocurrencia_avanzada))  # No usar head(50) si hay menos
  
  grafo_semantico <- coocurrencia_avanzada %>%
    head(n_max) %>%
    graph_from_data_frame(directed = FALSE)

  # M√©tricas
  V(grafo_semantico)$degree <- degree(grafo_semantico)
  V(grafo_semantico)$betweenness <- betweenness(grafo_semantico)
  V(grafo_semantico)$closeness <- closeness(grafo_semantico)
  
  # Comunidades
  comunidades <- cluster_fast_greedy(grafo_semantico)
  V(grafo_semantico)$community <- membership(comunidades)
  
  # Visualizaci√≥n
  p_red_semantica <- ggraph(grafo_semantico, layout = "stress") +
    geom_edge_link(aes(width = n, alpha = n), color = "gray70") +
    geom_node_point(aes(size = degree, color = factor(community)), alpha = 0.8) +
    geom_node_text(aes(label = name, size = degree), 
                   repel = TRUE, point.padding = 0.3, max.overlaps = 15) +
    scale_edge_width(range = c(0.5, 3), guide = "none") +
    scale_edge_alpha(range = c(0.3, 0.8), guide = "none") +
    scale_size_continuous(range = c(3, 8), guide = "none") +
    scale_color_brewer(type = "qual", palette = "Set3", name = "Comunidad") +
    labs(
      title = "Red Sem√°ntica de Conceptos Clave",
      subtitle = "Tama√±o = grado de conexi√≥n, Color = comunidad sem√°ntica",
      caption = "Basada en coocurrencia de palabras en documentos"
    ) +
    theme_void() +
    theme(
      plot.title = element_text(size = 14, face = "bold"),
      legend.position = "bottom"
    )
  
  print(p_red_semantica)
  
  # An√°lisis de centralidad
  centralidad_palabras <- tibble(
    palabra = V(grafo_semantico)$name,
    grado = V(grafo_semantico)$degree,
    intermediacion = V(grafo_semantico)$betweenness,
    cercania = V(grafo_semantico)$closeness,
    comunidad = V(grafo_semantico)$community
  ) %>%
    arrange(desc(grado))
  
} else {
  log_proceso("No hay suficientes coocurrencias para construir la red sem√°ntica", "WARNING")
}
```

```{r, include=FALSE}
# =============================================================================
# 9. AN√ÅLISIS MULTIVARIADO Y CLUSTERING
# =============================================================================

log_proceso("Realizando an√°lisis multivariado")

# Preparar matriz de caracter√≠sticas por documento
caracteristicas_docs <- tokens %>%
  # Seleccionar palabras m√°s informativas
  filter(word %in% freq_detallada$word[freq_detallada$rango <= 50]) %>%
  count(ubicacion, word) %>%
  # Aplicar TF-IDF
  bind_tf_idf(word, ubicacion, n) %>%
  select(ubicacion, word, tf_idf) %>%
  pivot_wider(names_from = word, values_from = tf_idf, values_fill = 0) %>%
  column_to_rownames("ubicacion")

# Eliminar columnas con varianza cero
caracteristicas_docs <- caracteristicas_docs[, apply(caracteristicas_docs, 2, var) > 0]

# PCA para reducci√≥n de dimensionalidad
if(ncol(caracteristicas_docs) > 3 && nrow(caracteristicas_docs) > 3) {
  
  pca_resultado <- prcomp(caracteristicas_docs, scale. = TRUE, center = TRUE)
  
  # Varianza explicada
  var_explicada <- summary(pca_resultado)$importance[2,] * 100
  
  # Datos PCA
  pca_datos <- pca_resultado$x[,1:min(5, ncol(pca_resultado$x))] %>%
    as_tibble(rownames = "ubicacion") %>%
    left_join(docs_index %>% select(ubicacion, anio), by = "ubicacion")
  
  # Filtrar a√±os con al menos 3 puntos para evitar warning en stat_ellipse
  conteo_por_anio <- pca_datos %>% count(anio)
  anios_validos <- conteo_por_anio %>% filter(n >= 3) %>% pull(anio)
  
  # Biplot PCA (solo a√±os v√°lidos)
  p_pca <- pca_datos %>%
    filter(anio %in% anios_validos) %>%
    ggplot(aes(x = PC1, y = PC2, color = factor(anio))) +
    geom_point(size = 3, alpha = 0.7) +
    stat_ellipse(type = "confidence", level = 0.68) +
    labs(
      title = "An√°lisis de Componentes Principales (PCA)",
      subtitle = sprintf("PC1: %.1f%% varianza, PC2: %.1f%% varianza", 
                         var_explicada[1], var_explicada[2]),
      x = sprintf("Componente Principal 1 (%.1f%%)", var_explicada[1]),
      y = sprintf("Componente Principal 2 (%.1f%%)", var_explicada[2]),
      color = "A√±o"
    ) +
    theme_minimal() +
    scale_color_brewer(type = "qual", palette = "Set2")
  
  plotly::ggplotly(p_pca)
  
  # Clustering jer√°rquico
  dist_docs <- dist(caracteristicas_docs, method = "euclidean")
  cluster_jerarquico <- hclust(dist_docs, method = "ward.D2")
  
  # Determinar n√∫mero √≥ptimo de clusters
  if(require(factoextra, quietly = TRUE)) {
    
    # M√©todo del codo
    p_codo <- fviz_nbclust(caracteristicas_docs, 
                           FUN = function(x, k) list(withinss = kmeans(x, k, nstart = 20)$tot.withinss),
                           method = "wss", k.max = min(8, nrow(caracteristicas_docs)-1))
    
    print(p_codo)
    
    # Clustering k-means
    k_optimo <- 3  # Ajustar seg√∫n resultado del m√©todo del codo
    kmeans_resultado <- kmeans(caracteristicas_docs, centers = k_optimo, nstart = 20)
    
    # A√±adir clusters a datos PCA
    pca_datos$cluster <- as.factor(kmeans_resultado$cluster)
    
    # Contar puntos por cluster para filtrar
    conteo_por_cluster <- pca_datos %>% count(cluster)
    clusters_validos <- conteo_por_cluster %>% filter(n >= 3) %>% pull(cluster)
    
    # Visualizaci√≥n con clusters v√°lidos
    p_pca_cluster <- pca_datos %>%
      filter(cluster %in% clusters_validos) %>%
      ggplot(aes(x = PC1, y = PC2, color = cluster, shape = factor(anio))) +
      geom_point(size = 4, alpha = 0.8) +
      stat_ellipse(aes(group = cluster), type = "confidence", level = 0.68) +
      labs(
        title = "Clustering de Documentos en Espacio PCA",
        subtitle = "Agrupaci√≥n por similitud sem√°ntica",
        x = sprintf("PC1 (%.1f%%)", var_explicada[1]),
        y = sprintf("PC2 (%.1f%%)", var_explicada[2]),
        color = "Cluster", shape = "A√±o"
      ) +
      theme_minimal() +
      scale_color_brewer(type = "qual", palette = "Dark2")
    
    print(p_pca_cluster)
  }
  
} else {
  log_proceso("Matriz de caracter√≠sticas insuficiente para ejecutar PCA")
}
```

```{r}
p_pca_cluster
```

```{r, include=FALSE}
# =============================================================================
# 10. M√âTRICAS DE CALIDAD Y VALIDACI√ìN
# =============================================================================

# M√©tricas de calidad del corpus
metricas_corpus <- list(
  # Estad√≠sticas b√°sicas
  total_documentos = nrow(docs_index),
  total_tokens = nrow(tokens),
  vocabulario_unico = n_distinct(tokens$word),
  
  # Diversidad l√©xica
  ratio_diversidad_global = n_distinct(tokens$word) / nrow(tokens),
  
  # Distribuci√≥n temporal
  cobertura_temporal = paste(range(docs_index$anio), collapse = "-"),
  
  # Calidad de procesamiento
  tasa_exito_limpieza = sum(docs_index$calidad_ok) / nrow(docs_index),
  reduccion_ruido_promedio = mean(docs_index$ratio_reduccion, na.rm = TRUE),
  
  # M√©tricas de contenido
  palabras_por_documento = mean(docs_index$num_palabras_limpia),
  oraciones_por_documento = mean(docs_index$num_oraciones),
  complejidad_lexica_promedio = mean(docs_index$complejidad_lexica, na.rm = TRUE),
  
  # Validaci√≥n ling√º√≠stica
  proporcion_espa√±ol = mean(docs_index$posible_espa√±ol),
  
  # M√©tricas de Zipf
  ajuste_zipf = summary(modelo_zipf)$r.squared,
  pendiente_zipf = coef(modelo_zipf)[2]
)

# Test de consistencia interna (si hay suficientes documentos)
if(nrow(docs_index) >= 10) {
  
  # Dividir corpus en mitades para validaci√≥n cruzada
  set.seed(123)
  muestra_1 <- sample(1:nrow(docs_index), size = floor(nrow(docs_index)/2))
  
  corpus_1 <- docs_index[muestra_1, ]
  corpus_2 <- docs_index[-muestra_1, ]
  
  # Tokenizar cada mitad
  tokens_1 <- tokenizar_avanzado(corpus_1, texto)
  tokens_2 <- tokenizar_avanzado(corpus_2, texto)
  
  # Calcular solapamiento de vocabulario
  vocab_1 <- unique(tokens_1$word)
  vocab_2 <- unique(tokens_2$word)
  
  solapamiento_vocab <- length(intersect(vocab_1, vocab_2)) / length(union(vocab_1, vocab_2))
  
  metricas_corpus$consistencia_vocabulario <- solapamiento_vocab
  
  log_proceso(sprintf("Consistencia de vocabulario entre mitades: %.3f", solapamiento_vocab))
}
```

```{r, include=FALSE}
# =============================================================================
# 10. M√âTRICAS DE CALIDAD Y VALIDACI√ìN
# =============================================================================

# M√©tricas de calidad del corpus
metricas_corpus <- list(
  # Estad√≠sticas b√°sicas
  total_documentos = nrow(docs_index),
  total_tokens = nrow(tokens),
  vocabulario_unico = n_distinct(tokens$word),
  
  # Diversidad l√©xica
  ratio_diversidad_global = n_distinct(tokens$word) / nrow(tokens),
  
  # Distribuci√≥n temporal
  cobertura_temporal = paste(range(docs_index$anio), collapse = "-"),
  
  # Calidad de procesamiento
  tasa_exito_limpieza = sum(docs_index$calidad_ok) / nrow(docs_index),
  reduccion_ruido_promedio = mean(docs_index$ratio_reduccion, na.rm = TRUE),
  
  # M√©tricas de contenido
  palabras_por_documento = mean(docs_index$num_palabras_limpia),
  oraciones_por_documento = mean(docs_index$num_oraciones),
  complejidad_lexica_promedio = mean(docs_index$complejidad_lexica, na.rm = TRUE),
  
  # Validaci√≥n ling√º√≠stica
  proporcion_espa√±ol = mean(docs_index$posible_espa√±ol),
  
  # M√©tricas de Zipf
  ajuste_zipf = summary(modelo_zipf)$r.squared,
  pendiente_zipf = coef(modelo_zipf)[2]
)

# Test de consistencia interna (si hay suficientes documentos)
if(nrow(docs_index) >= 10) {
  
  # Dividir corpus en mitades para validaci√≥n cruzada
  set.seed(123)
  muestra_1 <- sample(1:nrow(docs_index), size = floor(nrow(docs_index)/2))
  
  corpus_1 <- docs_index[muestra_1, ]
  corpus_2 <- docs_index[-muestra_1, ]
  
  # Tokenizar cada mitad
  tokens_1 <- tokenizar_avanzado(corpus_1, texto)
  tokens_2 <- tokenizar_avanzado(corpus_2, texto)
  
  # Calcular solapamiento de vocabulario
  vocab_1 <- unique(tokens_1$word)
  vocab_2 <- unique(tokens_2$word)
  
  solapamiento_vocab <- length(intersect(vocab_1, vocab_2)) / length(union(vocab_1, vocab_2))
  
  metricas_corpus$consistencia_vocabulario <- solapamiento_vocab
  
  log_proceso(sprintf("Consistencia de vocabulario entre mitades: %.3f", solapamiento_vocab))
}
```

## Conclusi√≥n
El an√°lisis textual sistem√°tico de los discursos oficiales del Estado mexicano durante la Guerra Cristera confirma que el lenguaje fue un instrumento central en la construcci√≥n simb√≥lica de la Iglesia cat√≥lica como enemiga del orden estatal, moral y nacional. A trav√©s de la asociaci√≥n constante de t√©rminos religiosos con categor√≠as de peligro, subversi√≥n o atraso, el Estado deline√≥ una frontera discursiva entre la modernidad legal-republicana y una religiosidad percibida como arcaica, reaccionaria y disruptiva.
Estas evidencias refuerzan la hip√≥tesis inicial al mostrar que no se trat√≥ simplemente de una pol√≠tica represiva aislada, sino de un proyecto discursivo coherente orientado a modelar subjetividades, disciplinar cuerpos e instituir un nuevo orden simb√≥lico. El uso articulado de categor√≠as como fanatismo, enemigo o castigo no solo justific√≥ legalmente la persecuci√≥n religiosa, sino que produjo subjetividades alineadas con un ideal de ciudadan√≠a secular, obediente y racional. As√≠, el discurso oficial no solo acompa√±√≥ las acciones del Estado, sino que oper√≥ como una tecnolog√≠a de poder en sentido foucaultiano, reconfigurando los l√≠mites de lo decible, lo leg√≠timo y lo nacional.
